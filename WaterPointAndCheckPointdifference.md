# Watermark vs Checkpoint in Spark Structured Streaming

## Key Difference

**Watermark â‰  Checkpoint**

| Concept        | What it is                    | Purpose                                    |
| -------------- | ----------------------------- | ------------------------------------------ |
| **Watermark**  | Event-time progress indicator | Handles late data & closes windows         |
| **Checkpoint** | Fault-tolerance mechanism     | Enables recovery & exactly-once processing |

---

## What is a Watermark?

A **watermark** tells Spark:

> *â€œIâ€™m confident that Iâ€™ve seen all events up to time **T** in **event time**.â€*

```python
df.withWatermark("timestamp", "10 minutes")
```

This means:

> *Wait up to **10 minutes** for late events.
> After that, older events are considered **too late** and will be dropped.*

---

## How Watermark Is Calculated

```
Current Watermark = Max Event Time Seen So Far - Watermark Delay
```

### Example

```python
clickstream_df.withWatermark("timestamp", "10 minutes")
```

**Batch 1 events**

| Event | Event Time |
| ----- | ---------- |
| A     | 10:00      |
| B     | 10:05      |
| C     | 10:03      |

```
Max event time = 10:05
Watermark = 10:05 - 10 minutes = 09:55
```

âœ” Events with timestamp **â‰¥ 09:55** â†’ Accepted
âŒ Events **< 09:55** â†’ Dropped as late

---

## Visual Timeline (Event Time)

```
09:50   09:55   10:00   10:05   10:10
|-------|-------|-------|-------|
```

**Processing Batch 1 (real time: 10:20)**

* Events received: `10:00, 10:05, 10:03, 09:57`
* Watermark = `09:55`
* `09:57` â†’ âœ… Accepted
* `09:50` â†’ âŒ Dropped

**Processing Batch 2**

* New max event time = `10:15`
* New watermark = `10:05`
* Events `< 10:05` â†’ Dropped

---

## What is a Checkpoint?

A **checkpoint** is a **persistent storage location** used to store:

* Kafka offsets
* Stateful aggregation data
* Metadata (batch IDs, configs)

```python
query = df.writeStream \
    .option("checkpointLocation", "/mnt/checkpoint/my_stream") \
    .start()
```

---

## Checkpoint Directory Structure

```
/mnt/checkpoint/my_stream/
â”œâ”€â”€ commits/     # Completed batches
â”œâ”€â”€ offsets/     # Kafka offsets
â”œâ”€â”€ state/       # Aggregation & window state
â””â”€â”€ metadata     # Stream configuration
```

---

## How Checkpointing Works

```text
10:00  Batch 0 â†’ offsets 0â€“1000 â†’ checkpointed
10:02  Batch 1 â†’ offsets 1001â€“2000 â†’ checkpointed
10:03  ğŸ’¥ Crash
10:05  Restart â†’ resume from offset 2001
```

âœ” No data loss
âœ” No duplicates
âœ” Exactly-once processing

---

## Side-by-Side Comparison

| Aspect      | Watermark           | Checkpoint          |
| ----------- | ------------------- | ------------------- |
| Tracks      | Event-time progress | Processing progress |
| Time domain | Event time          | Processing time     |
| Purpose     | Late data handling  | Fault tolerance     |
| Storage     | In-memory           | Persistent          |
| Decides     | Drop/accept events  | Resume point        |
| Example     | `09:55`             | Kafka offset `2000` |

---

## How Watermark and Checkpoint Work Together

```python
query = spark.readStream \
    .format("kafka") \
    .option("subscribe", "clicks") \
    .load() \
    .selectExpr("CAST(value AS STRING)") \
    .withWatermark("event_timestamp", "15 minutes") \
    .groupBy(
        window("event_timestamp", "5 minutes"),
        "user_id"
    ) \
    .agg(count("*").alias("event_count")) \
    .writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoint/clicks") \
    .trigger(processingTime="2 minutes") \
    .start()
```

* **Watermark** â†’ Decides which events are late
* **Checkpoint** â†’ Ensures recovery & exactly-once writes

---

## Common Misconceptions

âŒ *Watermark is last checkpoint time*
âœ… Watermark is based on **event timestamps**

âŒ *Checkpoint depends on watermark*
âœ… Checkpoint happens **every batch**

âŒ *Watermark tracks arrival time*
âœ… Watermark tracks **event time only**

âŒ *Watermark = current time âˆ’ delay*
âœ… Watermark = **max event time âˆ’ delay**

---

## How Does Spark Know an Event Is Late?

**It does NOT track arrival time.**

Spark only checks:

1. **Maximum event timestamp seen so far**
2. **Is this eventâ€™s timestamp â‰¥ watermark?**

### Example

```text
Max event time seen = 10:05
Watermark = 09:55

Event timestamp = 09:45 â†’ âŒ Dropped
Event timestamp = 10:00 â†’ âœ… Accepted
```

Even if the 09:45 event arrives *now*, it is still dropped.

---

## Mental Model

* **Watermark:**
  *â€œAm I willing to wait for this event based on its timestamp?â€*

* **Checkpoint:**
  *â€œWhere do I resume if the job crashes?â€*

---

## Final Summary

âœ” Watermark controls **late data handling**
âœ” Checkpoint ensures **fault tolerance**
âœ” Both are required for **correct streaming pipelines**

---
