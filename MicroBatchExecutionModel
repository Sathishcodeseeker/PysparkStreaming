# Micro-Batch Execution Model (Spark Structured Streaming)

Each trigger initiates **one atomic micro-batch** that goes through **four sequential stages**.

---

## High-Level Flow

```
TRIGGER FIRES
     ↓
┌──────────────────────────────────────────┐
│        MICRO-BATCH (Atomic Unit)         │
├──────────────────────────────────────────┤
│ 1. READ        → Fetch new data          │
│ 2. EXECUTE     → Apply transformations  │
│ 3. WRITE       → Persist results         │
│ 4. CHECKPOINT  → Commit state & offsets │
└──────────────────────────────────────────┘
     ↓
NEXT TRIGGER (waits for completion)
```

---

## Stage 1: READ (I/O Bound)

```python
# Reading from Kafka (LAZY until execution)
clickstream_df = (
    spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "broker:9092")
        .option("subscribe", "clicks")
        .option("maxOffsetsPerTrigger", 10000)  # Controls batch size
        .load()
)
```

### What Happens

* Queries Kafka for new offsets since the last checkpoint
* Determines batch boundaries (e.g., offsets `1000–11000`)
* **No data is pulled yet** (Spark is lazy)

---

## Stage 2: EXECUTE (CPU / Memory Bound)

```python
processed_df = (
    clickstream_df
        .select(from_json(col("value").cast("string"), schema).alias("data"))
        .select("data.*")
        .withWatermark("timestamp", "10 minutes")
        .groupBy(
            window(col("timestamp"), "5 minutes"),
            col("user_id")
        )
        .agg(
            count("*").alias("event_count"),
            sum("purchase_amount").alias("total_spend")
        )
)
```

### What Happens

* Logical plan is built (still lazy)
* When write starts, Spark:

  * Reads data from Kafka
  * Deserializes JSON
  * Applies watermark logic
  * Performs stateful aggregations
  * Prepares output for writing

---

## Stage 3: WRITE (I/O Bound — Often the Bottleneck)

```python
query = (
    processed_df.writeStream
        .foreachBatch(write_to_feature_store)
        .trigger(processingTime="2 minutes")
        .option("checkpointLocation", "/checkpoint")
        .start()
)

def write_to_feature_store(batch_df, batch_id):
    batch_df.write \
        .format("feast") \
        .mode("append") \
        .save()
```

### What Happens

* Results are written to Delta / Feature Store / Database
* Network I/O and serialization dominate
* **Entire micro-batch blocks until write completes**

---

## Stage 4: CHECKPOINT

```python
# Automatically handled by Spark
```

### What Happens

* Offsets are committed
* State snapshots are saved
* Batch is marked **successfully complete**
* **Only happens after a successful write**

---

## Time Analysis Example

```python
query = (
    df.writeStream
        .foreachBatch(write_to_feature_store)
        .trigger(processingTime="2 minutes")
        .start()
)
```

### Scenario 1: Fast Processing (Keeps Up)

```
10:00:00 - Trigger fires
10:00:05 - Read complete
10:00:25 - Execute complete
10:00:40 - Write complete
10:00:41 - Checkpoint saved
```

✅ Processing finishes **before next trigger**

---

### Scenario 2: Slow Writes (Backlog Builds)

```
10:00:00 - Trigger fires
10:03:25 - Write completes (3+ minutes)
10:03:26 - Next trigger fires immediately
```

⚠️ System falls behind — backlog accumulates

---

## Optimization Strategies

### 1. Parallel Writes inside `foreachBatch`

```python
def write_to_multiple_sinks(batch_df, batch_id):
    from concurrent.futures import ThreadPoolExecutor

    batch_df.cache()

    def write_delta():
        batch_df.write.format("delta").mode("append").save("/delta/path")

    def write_feature_store():
        batch_df.write.format("feast").save()

    def write_metrics():
        batch_df.write.jdbc(url="...", table="metrics")

    with ThreadPoolExecutor(max_workers=3) as executor:
        executor.map(lambda f: f(), [
            write_delta,
            write_feature_store,
            write_metrics
        ])

    batch_df.unpersist()
```

---

### 2. Optimize Feature Store Writes

* Repartition before write
* Increase write parallelism
* Batch records
* Use staging + async ingestion

---

### 3. Align Trigger with Write Time

```python
.trigger(processingTime="6 minutes")
```

✔️ Trigger interval should exceed slowest stage

---

### 4. Monitor Streaming Health

```python
query = spark.streams.active[0]

for p in query.recentProgress:
    if p["processedRowsPerSecond"] < p["inputRowsPerSecond"]:
        print("⚠️ Processing is lagging behind ingestion")
```

---

## Real-World Architecture Pattern (Dual Path)

### Fast Path (Low Latency)

```python
.clickstream_df
.groupBy(window("timestamp", "1 minute"), "user_id")
.writeStream
.format("delta")
.trigger(processingTime="30 seconds")
```

### Slow Path (Heavy Feature Engineering)

```python
.writeStream
.foreachBatch(write_enriched_features)
.trigger(processingTime="5 minutes")
```

---

## Key Takeaways

* ✅ Micro-batches are **strictly sequential**
* ✅ Write stage is usually the bottleneck
* ✅ Next trigger waits for full completion
* ✅ Optimize writes, not just compute
* ✅ Match trigger interval to slowest stage
* ✅ Monitor lag continuously

**Balance trigger frequency with write performance to avoid backlog buildup.**
