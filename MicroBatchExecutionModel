The Micro-Batch Execution Model
Each trigger initiates a single atomic micro-batch with three sequential stages:
TRIGGER FIRES
    ↓
┌─────────────────────────────────────────────────────┐
│              MICRO-BATCH (Atomic Unit)              │
├─────────────────────────────────────────────────────┤
│  Stage 1: READ (fetch new data based on offsets)    │
│      ↓                                              │
│  Stage 2: EXECUTE (apply transformations/logic)     │
│      ↓                                              │
│  Stage 3: WRITE (persist to sink/feature store)     │
│      ↓                                              │
│  Stage 4: CHECKPOINT (commit offsets + state)       │
└─────────────────────────────────────────────────────┘
    ↓
NEXT TRIGGER (waits for completion)
Detailed Breakdown
Stage 1: READ (I/O Bound)
python# Reading from Kafka - this is LAZY until execution
clickstream_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "broker:9092") \
    .option("subscribe", "clicks") \
    .option("maxOffsetsPerTrigger", 10000)  # Controls batch size
    .load()
What happens:

Query Kafka for new offsets since last checkpoint
Determine batch boundaries (e.g., offsets 1000-11000)
Does NOT actually pull data yet (Spark is lazy)

Stage 2: EXECUTE (CPU/Memory Bound)
python# Transformations build an execution plan
processed_df = clickstream_df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(window(col("timestamp"), "5 minutes"), col("user_id")) \
    .agg(
        count("*").alias("event_count"),
        sum("purchase_amount").alias("total_spend")
    )
What happens:

Spark builds a logical plan (lazy)
When write action triggers, physical execution begins:

Actually reads data from Kafka
Deserializes JSON
Applies watermark logic
Performs aggregations (using state store)
Prepares results for writing



Stage 3: WRITE (I/O Bound - Often the Bottleneck)
pythonquery = processed_df.writeStream \
    .foreachBatch(write_to_feature_store) \
    .trigger(processingTime="2 minutes") \
    .option("checkpointLocation", "/checkpoint") \
    .start()

def write_to_feature_store(batch_df, batch_id):
    # This is where the HEAVY I/O happens
    batch_df.write \
        .format("feast") \
        .mode("append") \
        .save()
What happens:

Write results to Delta Lake / Feature Store / Database
This is often the slowest part (network I/O, serialization)
The entire micro-batch blocks here until write completes

Stage 4: CHECKPOINT
python# Automatically handled by Spark
# Only happens AFTER successful write
What happens:

Commit processed offsets to checkpoint
Save state snapshots
Mark batch as complete

Time Analysis Example
Let's say you have a 2-minute trigger but different stage durations:
python# Trigger every 2 minutes
query = df.writeStream \
    .foreachBatch(write_to_feature_store) \
    .trigger(processingTime="2 minutes") \
    .start()
```

### Scenario 1: Fast Processing (Under Trigger Interval)
```
10:00:00 - Trigger 1 fires
10:00:05 - Read complete (5 sec)
10:00:25 - Execute complete (20 sec)
10:00:40 - Write to feature store complete (15 sec)
10:00:41 - Checkpoint saved (1 sec)
         - Total: 41 seconds
         
10:02:00 - Trigger 2 fires (waits until trigger interval)
10:02:05 - Read complete
10:02:30 - Execute complete
10:02:50 - Write complete
10:02:51 - Checkpoint saved
```

**✅ System keeps up** - processing completes before next trigger

### Scenario 2: Slow Feature Store Writes (Exceeds Trigger)
```
10:00:00 - Trigger 1 fires
10:00:05 - Read complete (5 sec)
10:00:25 - Execute complete (20 sec)
10:03:25 - Write to feature store complete (3 min - SLOW!)
10:03:26 - Checkpoint saved (1 sec)
         - Total: 3 min 26 sec
         
10:03:26 - Trigger 2 fires IMMEDIATELY (can't wait, already past 10:02:00)
10:03:31 - Read complete
10:03:51 - Execute complete
10:06:51 - Write to feature store complete
10:06:52 - Checkpoint saved
⚠️ System falling behind - backlog building up
How They Work Hand-in-Hand: Optimization Strategies
1. Parallel Writes with foreachBatch
pythondef write_to_multiple_sinks(batch_df, batch_id):
    """
    Even though stages are sequential, you can parallelize 
    writes within the write stage
    """
    from concurrent.futures import ThreadPoolExecutor
    
    # Cache to avoid recomputation
    batch_df.cache()
    
    def write_to_delta():
        batch_df.write.format("delta").mode("append").save("/delta/path")
    
    def write_to_feature_store():
        batch_df.write.format("feast").save()
    
    def write_to_metrics_db():
        batch_df.write.jdbc(url="...", table="metrics")
    
    # Execute writes in parallel
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [
            executor.submit(write_to_delta),
            executor.submit(write_to_feature_store),
            executor.submit(write_to_metrics_db)
        ]
        
        # Wait for all to complete
        for future in futures:
            future.result()
    
    batch_df.unpersist()

query = df.writeStream \
    .foreachBatch(write_to_multiple_sinks) \
    .trigger(processingTime="2 minutes") \
    .start()
2. Optimize Feature Store Writes
pythondef optimized_feature_store_write(batch_df, batch_id):
    """
    Strategies to speed up feature store writes
    """
    # Strategy 1: Partition data before writing
    batch_df \
        .repartition(10, "user_id") \
        .write \
        .format("feast") \
        .option("parallelism", "10") \
        .save()
    
    # Strategy 2: Batch writes instead of row-by-row
    batch_df \
        .coalesce(1) \
        .write \
        .format("feast") \
        .option("batchSize", "10000") \
        .save()
    
    # Strategy 3: Write to staging, async process to feature store
    batch_df.write.format("delta").mode("append").save("/staging")
    # Separate process moves from staging to feature store
3. Adjust Trigger Based on Write Performance
python# If feature store writes take 5 minutes on average
# Set trigger to 6 minutes to avoid backlog
query = df.writeStream \
    .foreachBatch(write_to_feature_store) \
    .trigger(processingTime="6 minutes")  # Give buffer time
    .start()
4. Monitor and Adapt
python# Check processing metrics
stream_query = spark.streams.active[0]

# Get recent progress
for progress in stream_query.recentProgress:
    print(f"Batch ID: {progress['batchId']}")
    print(f"Input rows: {progress['numInputRows']}")
    print(f"Processing time: {progress['processedRowsPerSecond']}")
    print(f"Trigger time: {progress['triggerExecution']}")
    
    # Check if falling behind
    if progress['processedRowsPerSecond'] < progress['inputRowsPerSecond']:
        print("⚠️ WARNING: Processing slower than ingestion!")
Real-World Architecture Pattern
python# Pattern: Dual-path architecture for slow feature stores

# Path 1: Fast path - real-time aggregations to Delta
fast_query = clickstream_df \
    .groupBy(window("timestamp", "1 minute"), "user_id") \
    .agg(count("*").alias("count")) \
    .writeStream \
    .format("delta") \
    .trigger(processingTime="30 seconds")  # Fast trigger
    .option("checkpointLocation", "/checkpoint/fast") \
    .start()

# Path 2: Slow path - enriched features to feature store
def write_enriched_features(batch_df, batch_id):
    # Heavy feature engineering
    enriched = batch_df.join(user_profile_table, "user_id")
    
    # Write to feature store (slow but OK with longer trigger)
    enriched.write.format("feast").save()

slow_query = clickstream_df \
    .writeStream \
    .foreachBatch(write_enriched_features) \
    .trigger(processingTime="5 minutes")  # Slower trigger, matches write time
    .option("checkpointLocation", "/checkpoint/slow") \
    .start()
Key Takeaways
✅ Sequential stages within each micro-batch (read → execute → write → checkpoint)
✅ Write stage often the bottleneck (especially with feature stores)
✅ Next trigger waits for current batch to complete all stages
✅ Optimize writes through partitioning, batching, parallel sinks
✅ Adjust triggers to match your slowest stage (typically writes)
✅ Monitor lag to detect when processing can't keep up with ingestion
The key is balancing trigger frequency with write performance to avoid backlog buildup!
